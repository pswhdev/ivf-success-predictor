{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "*   Evaluate missing data\n",
    "*   Clean data\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* outputs/datasets/collection/FertilityTrSeatmentData.csv.gz\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Generate cleaned Train and Test sets, both saved under outputs/datasets/cleaned\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "\n",
    "####  Data Cleaning Pipeline\n",
    "\n",
    "* Filter data to keep only entries with \"Treatment - IVF\" using:\n",
    "    - filter_ivf\n",
    "\n",
    "* Drop rows with 'Live birth occurence' value 1 and 'Embryos transferred' 0 using\n",
    "    - drop_erroneous\n",
    "\n",
    "* Drop Columns using:\n",
    "    - drop_columns\n",
    "  ```\n",
    "  ['Total number of previous DI cycles',\n",
    "  'Main reason for producing embroys storing eggs',\n",
    "  'Type of treatment - IVF or DI',\n",
    "  'Donated embryo',\n",
    "  'Eggs thawed (0/1)',\n",
    "  'Year of treatment',\n",
    "  'Number of live births',\n",
    "  'Embryos stored for use by patient',\n",
    "  'Fresh eggs stored (0/1)',\n",
    "  'Heart three birth congenital abnormalities',\n",
    "  'Heart two birth congenital abnormalities',\n",
    "  'Heart three delivery date',\n",
    "  'Heart three sex',\n",
    "  'Heart three birth weight',\n",
    "  'Heart three weeks gestation',\n",
    "  'Heart three birth outcome',\n",
    "  'Heart one birth congenital abnormalities',\n",
    "  'Heart two birth weight',\n",
    "  'Heart two delivery date',\n",
    "  'Heart two sex',\n",
    "  'Heart two weeks gestation',\n",
    "  'Heart two birth outcome',\n",
    "  'Heart one birth weight',\n",
    "  'Heart one weeks gestation',\n",
    "  'Heart one delivery date',\n",
    "  'Heart one sex',\n",
    "  'Heart one birth outcome',\n",
    "  'Number of foetal sacs with fetal pulsation',\n",
    "  'Early outcome',\n",
    "  'Partner ethnicity',\n",
    "  'Partner Type']\n",
    "  ```\n",
    "\n",
    "* Standardize datatype of \"Total number of previous pregnancies - IVF and DI\" and \"Total number of previous live births - IVF or DI\" and inpute 0 for missing values using the following imputers:\n",
    "    - convert_to_numeric\n",
    "    - zeros\n",
    "    - convert_to_int\n",
    "\n",
    "* Input \"Sperm Source\" using\n",
    "    - fill_sperm_source\n",
    "\n",
    "* Process and clean data on \"Date of embryo transfer\" using the following imputers:\n",
    "    - dot_to_int_999\n",
    "    - replace_missing_values\n",
    "    - append_cycle_type\n",
    "\n",
    "* Clean column 'Embryos transferred from eggs micro-injected' using:\n",
    "    - micro_injected\n",
    "\n",
    "* Input Donor age using:\n",
    "    - donor_age\n",
    "\n",
    "* Append 'e' to '1' in the 'Embryos transferred' column when a single embryo transfer was elective using:\n",
    "    - e_flagging\n",
    "\n",
    "* Annotate the value 0 in relevant columns using:\n",
    "    - type_of_cycle\n",
    "\n",
    "* Convert columns with data type float to data type integer using:\n",
    "    - float_to_int\n",
    "\n",
    "* Drop rows with placeholder values '999' using:\n",
    "    - drop_999 = DropRowsWith999()\n",
    "\n",
    "* Drop remaining rows containing missing values using:\n",
    "    - drop_missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the working directory from its current folder to its parent folder\n",
    "* Access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the parent of the current directory the new current directory:\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"A new current directory has been set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the DataFrame from the compressed CSV file\n",
    "df = pd.read_csv('outputs/datasets/collection/FertilityTreatmentData.csv.gz')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Number of empty entries followed by the unique values and data type at each column:\\n\")\n",
    "\n",
    "for column in df.columns:\n",
    "    # Check how many empty fields there are in each column\n",
    "    empty_fields_count = df[column].isnull().sum()\n",
    "    # Check unique values there are in each column\n",
    "    unique_values = df[column].unique()\n",
    "    # Check data type of each column\n",
    "    data_type = df[column].dtype\n",
    "    \n",
    "    print (f\"- {column}: {empty_fields_count}, {unique_values}, {data_type}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "pandas_report = ProfileReport(df=df, minimal=True)\n",
    "pandas_report.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the distribution and shape of a variable with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
    "vars_with_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "if vars_with_missing_data:\n",
    "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
    "    profile.to_notebook_iframe()\n",
    "else:\n",
    "    print(\"There are no variables with missing data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Missing Data Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Custom function to display missing data levels in a DataFrame, it shows the absolute levels, relative levels and data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateMissingData(df):\n",
    "    missing_data_absolute = df.isnull().sum()\n",
    "    missing_data_percentage = round(missing_data_absolute/len(df)*100, 2)\n",
    "    df_missing_data = (pd.DataFrame(\n",
    "                            data={\"RowsWithMissingData\": missing_data_absolute,\n",
    "                                   \"PercentageOfDataset\": missing_data_percentage,\n",
    "                                   \"DataType\": df.dtypes}\n",
    "                                    )\n",
    "                          .sort_values(by=['PercentageOfDataset'], ascending=False)\n",
    "                          .query(\"PercentageOfDataset > 0\")\n",
    "                          )\n",
    "\n",
    "    return df_missing_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check missing data levels for the collected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TrainSet, TestSet, _, __ = train_test_split(\n",
    "                                        df,\n",
    "                                        df['Live birth occurrence'],\n",
    "                                        test_size=0.2,\n",
    "                                        random_state=0)\n",
    "\n",
    "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_data = EvaluateMissingData(TrainSet)\n",
    "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
    "df_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Filter the Dataset to include only IVF treatments by filtering 'Main reason for producing embroys storing eggs' only for \"Treatment - IVF\"\n",
    "\n",
    "* Drop likely erroneous entries. Remove rows where 'Live birth occurrence' has value 1 and 'Embryos transferred' has value 0.\n",
    "\n",
    "* Drop columns that have missing data and don't add relevant information for the analysis:\n",
    "    - 'Total number of previous DI cycles',\n",
    "    - 'Main reason for producing embroys storing eggs' (after filtering the df for 'Treatment - IVF')\n",
    "    - 'Type of treatment - IVF or DI' (will have only IVF values after filtering the df)\n",
    "    - 'Donated embryo',\n",
    "    - 'Eggs thawed (0/1)',\n",
    "    - 'Year of treatment',\n",
    "    - 'Number of live births',\n",
    "    - 'Embryos stored for use by patient',\n",
    "    - 'Fresh eggs stored (0/1)',\n",
    "    - 'Heart three birth congenital abnormalities',\n",
    "    - 'Heart two birth congenital abnormalities',\n",
    "    - 'Heart three delivery date',\n",
    "    - 'Heart three sex',\n",
    "    - 'Heart three birth weight',\n",
    "    - 'Heart three weeks gestation',\n",
    "    - 'Heart three birth outcome',\n",
    "    - 'Heart one birth congenital abnormalities',\n",
    "    - 'Heart two birth weight',\n",
    "    - 'Heart two delivery date',\n",
    "    - 'Heart two sex',\n",
    "    - 'Heart two weeks gestation',\n",
    "    - 'Heart two birth outcome',\n",
    "    - 'Heart one birth weight',\n",
    "    - 'Heart one weeks gestation',\n",
    "    - 'Heart one delivery date',\n",
    "    - 'Heart one sex',\n",
    "    - 'Heart one birth outcome',\n",
    "    - 'Number of foetal sacs with fetal pulsation',\n",
    "    - 'Early outcome',\n",
    "    - 'Partner ethnicity'\n",
    "    - 'Partner Type'\n",
    "\n",
    "* \"Total number of previous pregnancies - IVF and DI\" and \"Total number of previous live births - IVF or DI\" columns need to have the data type standardized and missing values should be inmputed with \"0\"\n",
    "\n",
    "* 'Sperm source' missing entries should be filled up with 'Donor' if there is a 'Sperm donor age at registration', otherwise, fill up with 'Partner'.\n",
    "\n",
    "* Process and clean data on \"Date of embryo transfer\" column: Convert float values to integers and handle NaNs.\n",
    "Replace the value 999 with 0, as these entries represent frozen cycles. Replace missing values with \"NT\" for \"No transfer\" if \"Embryos transferred\" is 0. Append strings based on \"Fresh cycle\" and \"Frozen cycle\" values.\n",
    "\n",
    "* Clean column 'Embryos transferred from eggs micro-injected': If the specific treatment type includes 'ICSI', then fill missing values with value from column 'Embryos transferred' , otherwise fill missing values with 0.\n",
    "\n",
    "* Impute missing values in the \"Egg donor age at registration\" and \"Sperm donor age at registration\" columns based on their respective source columns (\"Egg source\" and \"Sperm source\"). If the source is \"Patient\" or \"Partner,\" fill the missing values using the \"Patient age at treatment\" and \"Partner age\" columns, respectively. After imputation, rename the columns to \"Patient/Egg provider age\" and \"Partner/Sperm provider age\" accordingly.\n",
    "\n",
    "* Append 'e' to '1' in the 'Embryos transferred' column when a single embryo transfer was elective to enhance clarity and analysis.\n",
    "\n",
    "* Annotate the value 0 in relevant columns to indicate whether it pertains to a frozen or fresh cycle,giving the value contextual meaning.\n",
    "\n",
    "* Convert columns with data type float to data type integer.\n",
    "\n",
    "* Drop rows with placeholder values '999'.\n",
    "\n",
    "* Drop all ramaining rows with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the Dataset to include only IVF treatments\n",
    "\n",
    "Since the costumer is interested in predicting the chance of success using IFV treatment, the first step is to filter the data and keep only entrances with \"Main reason for producing embroys storing eggs\" with the value of \"Treatment - IVF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FilterIVFTreatments(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.query(\"`Main reason for producing embroys storing eggs` == 'Treatment - IVF'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe and apply drop_erroneous to the selected variables in the TrainSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ivf = FilterIVFTreatments()\n",
    "df_filter_ivf = filter_ivf.fit_transform(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter_ivf['Main reason for producing embroys storing eggs'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ivf = FilterIVFTreatments()\n",
    "TrainSet_cleaned, TestSet_cleaned = filter_ivf.transform(TrainSet), filter_ivf.transform(TestSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows with 'Live birth occurrence' value 1 and 'Embryos transferred' 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is not possible to have a successfull treatment with a Live occurence without having had embryos transferred, these entries cannot be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropErroneousEntries(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.drop(X[(X['Live birth occurrence'] == 1) & (X['Embryos transferred'] == 0)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe and apply drop_erroneous to the selected variables in the TrainSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_erroneous = DropErroneousEntries()\n",
    "df_drop_erroneous= drop_erroneous.fit_transform(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataFrame after applying DropErroneousEntries:\")\n",
    "print(df_drop_erroneous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_erroneous = DropErroneousEntries()\n",
    "TrainSet_cleaned, TestSet_cleaned = drop_erroneous.transform(TrainSet_cleaned), drop_erroneous.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns that have missing data and/or don't add relevant information for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'Total number of previous DI cycles',\n",
    "    'Main reason for producing embroys storing eggs',\n",
    "    'Type of treatment - IVF or DI',\n",
    "    'Donated embryo',\n",
    "    'Eggs thawed (0/1)',\n",
    "    'Year of treatment',\n",
    "    'Number of live births',\n",
    "    'Embryos stored for use by patient',\n",
    "    'Fresh eggs stored (0/1)',\n",
    "    'Heart three birth congenital abnormalities',\n",
    "    'Heart two birth congenital abnormalities',\n",
    "    'Heart three delivery date',\n",
    "    'Heart three sex',\n",
    "    'Heart three birth weight',\n",
    "    'Heart three weeks gestation',\n",
    "    'Heart three birth outcome',\n",
    "    'Heart one birth congenital abnormalities',\n",
    "    'Heart two birth weight',\n",
    "    'Heart two delivery date',\n",
    "    'Heart two sex',\n",
    "    'Heart two weeks gestation',\n",
    "    'Heart two birth outcome',\n",
    "    'Heart one birth weight',\n",
    "    'Heart one weeks gestation',\n",
    "    'Heart one delivery date',\n",
    "    'Heart one sex',\n",
    "    'Heart one birth outcome',\n",
    "    'Number of foetal sacs with fetal pulsation',\n",
    "    'Early outcome',\n",
    "    'Partner ethnicity',\n",
    "    'Partner Type'\n",
    "    ]\n",
    "\n",
    "print(f\"* {len(columns_to_drop)} variables to drop \\n\\n\"\n",
    "    f\"{columns_to_drop}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply imputation approach to the selected variables in the TrainSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropFeatures\n",
    "print(TrainSet_cleaned.columns)\n",
    "drop_columns = DropFeatures(features_to_drop=columns_to_drop)\n",
    "df_dropped_columns = drop_columns.fit_transform(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped_columns.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to the Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropFeatures\n",
    "\n",
    "drop_columns = DropFeatures(features_to_drop=columns_to_drop)\n",
    "drop_columns.fit(TrainSet_cleaned)\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = drop_columns.transform(TrainSet_cleaned), drop_columns.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling \"Total number of previous pregnancies - IVF and DI\" and \"Total number of previous live births - IVF or DI\" columns\n",
    "\n",
    "* Turn values to numeric\n",
    "* Impute missing values with \"0\".\n",
    "* Standardize values on column by converting the data type to integers.\n",
    "* Replace('>3', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn values to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ConvertToNumeric(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in self.columns:\n",
    "            # Replace '>3' with 4\n",
    "            X[col] = X[col].replace('>3', 4)\n",
    "            # Convert to numeric\n",
    "            X[col] = pd.to_numeric(X[col])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe and apply convert_to_numeric to the selected variables in the TrainSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_numeric = ConvertToNumeric(columns=['Total number of previous pregnancies - IVF and DI', 'Total number of previous live births - IVF or DI'])\n",
    "df_prev_preg_births_to_numeric = convert_to_numeric.fit_transform(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_prev_preg_births_to_numeric['Total number of previous pregnancies - IVF and DI'].dtype)\n",
    "print(df_prev_preg_births_to_numeric['Total number of previous live births - IVF or DI'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_numeric = ConvertToNumeric(columns=['Total number of previous pregnancies - IVF and DI', 'Total number of previous live births - IVF or DI'])\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = convert_to_numeric.transform(TrainSet_cleaned), convert_to_numeric.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill missing values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import ArbitraryNumberImputer\n",
    "\n",
    "# Fill missing values with 0 for specified columns\n",
    "zeros_imputer = ArbitraryNumberImputer(arbitrary_number=0, variables=[\n",
    "    'Total number of previous pregnancies - IVF and DI',\n",
    "    'Total number of previous live births - IVF or DI'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe and apply convert_to_int to the selected variables in the TrainSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_prev_preg_births_zero_imputed = zeros_imputer.fit_transform(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare columns before and after transformation\n",
    "def compare_columns(df_original, df_cleaned, columns):\n",
    "    comparison_dict = {}\n",
    "    summary_list = []\n",
    "\n",
    "    for column in columns:\n",
    "        comparison_dict[f'{column}_Before_Cleaning'] = df_original[column]\n",
    "        comparison_dict[f'{column}_After_Cleaning'] = df_cleaned[column]\n",
    "            \n",
    "        before_unique = df_original[column].unique()\n",
    "        after_unique = df_cleaned[column].unique()\n",
    "        before_missing = df_original[column].isna().sum()\n",
    "        after_missing = df_cleaned[column].isna().sum()\n",
    "        \n",
    "        summary_list.append({\n",
    "            'Column': column,\n",
    "            'Before Unique Values': before_unique,\n",
    "            'After Unique Values': after_unique,\n",
    "            'Before Missing Entries': before_missing,\n",
    "            'After Missing Entries': after_missing\n",
    "        })\n",
    "        \n",
    "    comparison_df = pd.DataFrame(comparison_dict)\n",
    "    summary_df = pd.DataFrame(summary_list)\n",
    "    \n",
    "    return comparison_df, summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to compare\n",
    "columns_to_compare = ['Total number of previous pregnancies - IVF and DI', 'Total number of previous live births - IVF or DI']\n",
    "\n",
    "# Compare the columns before and after cleaning\n",
    "comparison_prev_preg_births_zero_imputed = compare_columns(TrainSet_cleaned, df_prev_preg_births_zero_imputed, columns_to_compare)\n",
    "print(comparison_prev_preg_births_zero_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_imputer = ArbitraryNumberImputer(arbitrary_number=0, variables=[\n",
    "    'Total number of previous pregnancies - IVF and DI',\n",
    "    'Total number of previous live births - IVF or DI'\n",
    "]).fit(TrainSet_cleaned)\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = zeros_imputer.transform(TrainSet_cleaned), zeros_imputer.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardize values on column by converting the data type to integers and replace('>3', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ConvertToIntegers(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in self.columns:\n",
    "            # Replace '>3' with 4 and convert to int\n",
    "            X[col] = X[col].replace('>3', 4).astype(float).astype(int)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe and apply convert_to_int to the selected variables in the TrainSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_int = ConvertToIntegers(['Total number of previous pregnancies - IVF and DI', 'Total number of previous live births - IVF or DI'])\n",
    "df_prev_preg_births_to_int = convert_to_int.fit_transform(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to compare\n",
    "columns_to_compare = ['Total number of previous pregnancies - IVF and DI', 'Total number of previous live births - IVF or DI']\n",
    "\n",
    "# Compare the columns before and after cleaning\n",
    "comparison_prev_preg_births_int = compare_columns(TrainSet_cleaned, df_prev_preg_births_to_int, columns_to_compare)\n",
    "print(comparison_prev_preg_births_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_int = ConvertToIntegers(['Total number of previous pregnancies - IVF and DI', 'Total number of previous live births - IVF or DI'])\n",
    "convert_to_int.fit(TrainSet_cleaned)\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = convert_to_int.transform(TrainSet_cleaned), convert_to_int.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean 'Sperm source' missing entries\n",
    "\n",
    "If there is a 'Sperm donor age at registration', input 'Donor'; otherwise, input 'Partner'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FillSpermSource(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['Sperm source'] = X.apply(self._fill_sperm_source, axis=1)\n",
    "        return X\n",
    "\n",
    "    def _fill_sperm_source(self, row):\n",
    "        if pd.isna(row['Sperm source']):\n",
    "            if not pd.isna(row['Sperm donor age at registration']):\n",
    "                return 'Donor'\n",
    "            else:\n",
    "                return 'Partner'\n",
    "        return row['Sperm source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe and apply fill_sperm_source to the selected variables in the TrainSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the transformer\n",
    "fill_sperm_source = FillSpermSource()\n",
    "df_filled_sperm_source = fill_sperm_source.fit_transform(TrainSet_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to compare\n",
    "columns_to_compare = ['Sperm source', 'Sperm donor age at registration']\n",
    "\n",
    "# Compare the columns before and after cleaning\n",
    "comparison_sperm_source = compare_columns(TrainSet_cleaned, df_filled_sperm_source, columns_to_compare)\n",
    "print(comparison_sperm_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_sperm_source = FillSpermSource()\n",
    "fill_sperm_source.fit(TrainSet_cleaned)\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = fill_sperm_source.transform(TrainSet_cleaned), fill_sperm_source.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling \"Date of embryo transfer\" column\n",
    "\n",
    "This column has to be handled using several custom transformers to:\n",
    "* Convert float values to integers and handle NaN values.\n",
    "* Replace the value 999 with 0. All these \"999\" entries are from frozen cycles and transfers from frozen cycles happen mostly on the day same day they are thawed.\n",
    "* Replace missing values based on the \"Embryos transferred\" column. If the value is 0, the missing entries need to be replaced by \"NT\" for \"No transfer\", meaning that the treatment didn't work.\n",
    "* Append strings based on the \"Fresh cycle\" and \"Frozen cycle\" values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a Custom Transformer to convert float values to integers, handle NaN values and replace the value 999 with 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Convert float values to integers and handle NaN values\n",
    "class ConvertToIntAndReplace999(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Fill NaN with -1 and convert to int\n",
    "        X['Date of embryo transfer'] = X['Date of embryo transfer'].fillna(-1).astype(int)\n",
    "        # Replace 999 with 0\n",
    "        X['Date of embryo transfer'] = X['Date of embryo transfer'].replace(999, 0)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the dot_to_int_999 to the selected variables in the TrainSet to verify the cumulative cleaning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the transformer\n",
    "dot_to_int_999 = ConvertToIntAndReplace999()\n",
    "df_dot_to_int_999 = dot_to_int_999.fit_transform(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to compare\n",
    "columns_to_compare = ['Date of embryo transfer']\n",
    "\n",
    "# Compare the columns before and after cleaning\n",
    "comparison_date_transf_to_int_999 = compare_columns(TrainSet_cleaned, df_dot_to_int_999, columns_to_compare)\n",
    "print(comparison_date_transf_to_int_999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_to_int_999 = ConvertToIntAndReplace999()\n",
    "dot_to_int_999.fit(TrainSet_cleaned)\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = dot_to_int_999.transform(TrainSet_cleaned), dot_to_int_999.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace missing values based on the \"Embryos transferred\" column.\n",
    "\n",
    "If the value is 0, the missing entries need to be replaced by \"NT\" for \"No transfer\", meaning that the treatment didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values based on the \"Embryos transferred\" column\n",
    "class ReplaceMissingValues(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['Date of embryo transfer'] = X.apply(self._replace_missing, axis=1)\n",
    "        return X\n",
    "\n",
    "    def _replace_missing(self, row):\n",
    "        value = row['Date of embryo transfer']\n",
    "        if value == -1 and row['Embryos transferred'] == 0:\n",
    "            return 'NT'\n",
    "        elif value == -1:\n",
    "            return 'Missing'\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the replace_missing_values to the selected variables in the TrainSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the transformer\n",
    "replace_missing_values = ReplaceMissingValues()\n",
    "df_replace_missing_values = replace_missing_values.fit_transform(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to compare\n",
    "columns_to_compare = ['Date of embryo transfer', 'Embryos transferred']\n",
    "\n",
    "# Compare the columns before and after cleaning\n",
    "comparison_date_transf_replace_missing_values = compare_columns(TrainSet_cleaned, df_replace_missing_values, columns_to_compare)\n",
    "print(comparison_date_transf_replace_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_missing_values = ReplaceMissingValues()\n",
    "replace_missing_values.fit(TrainSet_cleaned)\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = replace_missing_values.transform(TrainSet_cleaned), replace_missing_values.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Append strings based on the \"Fresh cycle\" and \"Frozen cycle\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append strings based on the \"Fresh cycle\" and \"Frozen cycle\" values\n",
    "class AppendCycleType(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['Date of embryo transfer'] = X.apply(self._append_cycle_type, axis=1)\n",
    "        return X\n",
    "\n",
    "    def _append_cycle_type(self, row):\n",
    "        value = row['Date of embryo transfer']\n",
    "        if value not in ['NT', 'Missing']:\n",
    "            if row['Fresh cycle'] == 1:\n",
    "                value = f\"{value} - fresh\"\n",
    "            elif row['Frozen cycle'] == 1:\n",
    "                value = f\"{value} - frozen\"\n",
    "            else:\n",
    "                value = f\"{value} - Mixed fresh/frozen\"\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the embryo_transfer to the selected variables in the TrainSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_cycle_type = AppendCycleType()\n",
    "df_appended_cycle_type = append_cycle_type.fit_transform(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to compare\n",
    "columns_to_compare = ['Date of embryo transfer', 'Embryos transferred', 'Fresh cycle', 'Frozen cycle']\n",
    "\n",
    "# Compare the columns before and after cleaning\n",
    "comparison_date_trans_cycle_type = compare_columns(TrainSet_cleaned, df_appended_cycle_type, columns_to_compare)\n",
    "print(comparison_date_trans_cycle_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_cycle_type = AppendCycleType()\n",
    "append_cycle_type.fit(TrainSet_cleaned)\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = append_cycle_type.transform(TrainSet_cleaned), append_cycle_type.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean column 'Embryos transferred from eggs micro-injected'\n",
    "\n",
    "If the specific treatment type includes 'ICSI', then fill missing values with value from column 'Embryos transferred' , otherwise fill missing values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MicroInjectedEmbryos(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Embryos transferred from eggs micro-injected imputation\n",
    "        missing_micro_injected = (X['Embryos transferred from eggs micro-injected'].isna())\n",
    "        ICSI = X['Specific treatment type'].str.contains('ICSI')\n",
    "        # Only replace missing values\n",
    "        X.loc[missing_micro_injected & ICSI, 'Embryos transferred from eggs micro-injected'] = X.loc[missing_micro_injected & ICSI, 'Embryos transferred']\n",
    "        X.loc[missing_micro_injected & ~ICSI, 'Embryos transferred from eggs micro-injected'] = 0\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the micro_injected to the selected variables in the TrainSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the transformer\n",
    "micro_injected = MicroInjectedEmbryos()\n",
    "df_micro_injected = micro_injected.fit_transform(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to compare\n",
    "columns_to_compare = ['Specific treatment type', 'Embryos transferred', 'Embryos transferred from eggs micro-injected']\n",
    "\n",
    "# Compare the columns before and after cleaning\n",
    "comparison_micro_injected_embryos = compare_columns(TrainSet_cleaned, df_micro_injected, columns_to_compare)\n",
    "print(comparison_micro_injected_embryos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_injected = MicroInjectedEmbryos()\n",
    "micro_injected.fit(TrainSet_cleaned)\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = micro_injected.transform(TrainSet_cleaned), micro_injected.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling 'Egg donor age at registration' and 'Sperm donor age at registration'\n",
    "\n",
    "Both of these columns have more than 90% missing data, but the missing data can be managed by checking the respective source columns ('Egg source' and 'Sperm source') to determine if the source is \"Donor\" or \"Patient/Partner\".\n",
    "\n",
    "**Egg donor age at registration:**\n",
    "- For missing fields in the \"Egg donor age at registration\" column, if the value in the 'Egg source' column is \"Patient\", then the field can be filled with the patient's age from the \"Patient age at treatment\" column.\n",
    "- After that, the column \"Egg donor age at registration\" is renamed to \"Patient/Egg provider age\".\n",
    "- The age in this dataset is represented as ranges, which are different between \"Patient age at treatment\" ('18-34', '35-37', '38-39', '40-42', '43-44', '45-50') and the original \"Egg donor age at registration\" ('<= 20', 'Between 21 and 25', 'Between 26 and 30', 'Between 31 and 35', '>35'). Therefore, the ranges need to be standardized.\n",
    "- Since the majority of values will come from the \"Patient age at treatment\", this column's ranges are used as the reference to adjust the \"Patient/Egg provider age\".\n",
    "\n",
    "**Sperm donor age at registration:**\n",
    "- For missing fields in the \"Sperm donor age at registration\" column, if the value in the 'Sperm source' column is \"Partner\", then the field can be filled with the partner's age from the \"Partner age\" column.\n",
    "- After that, the column \"Sperm donor age at registration\" is renamed to \"Partner/Sperm provider age\".\n",
    "- The age in this dataset is represented as ranges, which are different between \"Partner age\" ('18-34', '35-37', '38-39', '40-42', '43-44', '45-50', '51-55', '56-60', '>60') and the original \"Sperm donor age at registration\" ('<= 20', 'Between 21 and 25', 'Between 26 and 30', 'Between 31 and 35', 'Between 36 and 40', 'Between 41 and 45', '>45'). Therefore, the ranges need to be standardized.\n",
    "- Since the majority of values will come from the \"Partner age\", this column's ranges are used as the reference to adjust the \"Partner/Sperm provider age\".\n",
    "- The column \"Partner age\" can then be dropped because the useful information will already be saved on the \"Partner/Sperm provider age\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DonorAgeImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Mapping from donor age ranges to patient/partner age ranges\n",
    "        self.egg_age_map = {\n",
    "            'Between 21 and 25': '18-34',\n",
    "            'Between 26 and 30': '18-34',\n",
    "            'Between 31 and 35': '18-34',\n",
    "            '>35': '38-39',\n",
    "            '<= 20': '18-34'\n",
    "        }\n",
    "        self.sperm_age_map = {\n",
    "            'Between 21 and 25': '18-34',\n",
    "            'Between 26 and 30': '18-34',\n",
    "            'Between 31 and 35': '18-34',\n",
    "            'Between 36 and 40': '38-39',\n",
    "            'Between 41 and 45': '43-44',\n",
    "            '>45': '45-50',\n",
    "            '<= 20': '18-34'\n",
    "        }\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Egg donor age imputation\n",
    "        X['Egg donor age at registration'] = X['Egg donor age at registration'].map(self.egg_age_map)\n",
    "        missing_egg_age = (X['Egg donor age at registration'].isna()) & (X['Egg source'] == 'Patient')\n",
    "        X.loc[missing_egg_age, 'Egg donor age at registration'] = X.loc[missing_egg_age, 'Patient age at treatment']\n",
    "        X.rename(columns={'Egg donor age at registration': 'Patient/Egg provider age'}, inplace=True)\n",
    "        \n",
    "        # Sperm donor age imputation\n",
    "        X['Sperm donor age at registration'] = X['Sperm donor age at registration'].map(self.sperm_age_map)\n",
    "        missing_sperm_age = (X['Sperm donor age at registration'].isna()) & (X['Sperm source'] == 'Partner')\n",
    "        X.loc[missing_sperm_age, 'Sperm donor age at registration'] = X.loc[missing_sperm_age, 'Partner age']\n",
    "        X.rename(columns={'Sperm donor age at registration': 'Partner/Sperm provider age'}, inplace=True)\n",
    "\n",
    "        # Drop the \"Partner age\" column\n",
    "        X.drop(columns=['Partner age'], inplace=True)\n",
    "        \n",
    "        # Ensure no duplicate columns\n",
    "        if X.columns.duplicated().any():\n",
    "            raise ValueError(\"Duplicate column names found after transformation\")\n",
    "        \n",
    "        return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the donor_age to the selected variables in the TrainSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_age = DonorAgeImputer()\n",
    "df_donor_age = donor_age.fit_transform(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Egg and Sperm donor age at registration Data Celaning evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CompareDataCleaning(df_original, df_cleaned, variable_map):\n",
    "    missing_values = {}\n",
    "    unique_values = {}\n",
    "\n",
    "    for original_var, cleaned_var in variable_map.items():\n",
    "        # Missing values\n",
    "        original_missing_count = df_original[original_var].isna().sum()\n",
    "        cleaned_missing_count = df_cleaned[cleaned_var].isna().sum()\n",
    "        original_missing_percent = (original_missing_count / len(df_original)) * 100\n",
    "        cleaned_missing_percent = (cleaned_missing_count / len(df_cleaned)) * 100\n",
    "\n",
    "        missing_values[original_var] = pd.DataFrame({\n",
    "            'Original Missing Count': [original_missing_count],\n",
    "            'Original Missing Percent': [original_missing_percent],\n",
    "            'Cleaned Missing Count': [cleaned_missing_count],\n",
    "            'Cleaned Missing Percent': [cleaned_missing_percent]\n",
    "        })\n",
    "\n",
    "        # Unique value counts\n",
    "        original_unique = df_original[original_var].value_counts(dropna=False)\n",
    "        cleaned_unique = df_cleaned[cleaned_var].value_counts(dropna=False)\n",
    "        unique_values[original_var] = pd.DataFrame({\n",
    "            'Original': original_unique,\n",
    "            'Cleaned': cleaned_unique\n",
    "        })\n",
    "\n",
    "    # Display results\n",
    "    for original_var, cleaned_var in variable_map.items():\n",
    "        print(\"\\n=====================================================================================\")\n",
    "        print(f\"Missing Values for {original_var} -> {cleaned_var}:\\n\")\n",
    "        print(missing_values[original_var])\n",
    "        print(f\"\\nUnique Values for {original_var} -> {cleaned_var}:\\n\")\n",
    "        print(unique_values[original_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of original to cleaned variables\n",
    "variable_map_donor = {\n",
    "    'Egg donor age at registration': 'Patient/Egg provider age',\n",
    "    'Sperm donor age at registration': 'Partner/Sperm provider age',\n",
    "}\n",
    "\n",
    "CompareDataCleaning(df_original=TrainSet_cleaned, df_cleaned=df_donor_age, variable_map=variable_map_donor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_age = DonorAgeImputer()\n",
    "donor_age.fit(TrainSet_cleaned)\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = donor_age.transform(TrainSet_cleaned), donor_age.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all float data type variables to data type integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloatToIntTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.float_vars = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Identify float columns\n",
    "        self.float_vars = X.select_dtypes(include='float').columns.tolist()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for var in self.float_vars:\n",
    "            X[var] = X[var].astype(int)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the FloatToIntTransformer to the selected variables in the TrainSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_to_int = FloatToIntTransformer()\n",
    "df_float_to_int = float_to_int.fit_transform(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types before transformation\n",
    "print(\"Data types before transformation:\")\n",
    "print(TrainSet_cleaned.dtypes)\n",
    "print(\"\\n\")\n",
    "print(\"========================================\")\n",
    "print(\"\\n\")\n",
    "# Check data types after transformation\n",
    "print(\"Data types after transformation:\")\n",
    "print(df_float_to_int.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_to_int = FloatToIntTransformer()\n",
    "float_to_int.fit(TrainSet_cleaned)\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = float_to_int.transform(TrainSet_cleaned), float_to_int.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicitly mark the transferred embryos that were electively selected\n",
    "\n",
    "To enhance the clarity of the 'Embryos transferred' column, an \"e\" will be appended to the 1 in 'Embryos transferred' when both 'Embryos transferred' and 'Elective single embryo transfer' columns have a value of 1. This will change the value to \"1e\".\n",
    "\n",
    "This adjustment is intended to indicate cases where a single embryo transfer was elective, thereby distinguishing it from situations where only one embryo was available for transfer.\n",
    "\n",
    "Explicitly marking elective single embryo transfers improves the analysis of the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EFlaggingTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        X['Embryos transferred'] = X.apply(self.append_e, axis=1)\n",
    "        return X\n",
    "    \n",
    "    def append_e(self, row):\n",
    "        if row['Embryos transferred'] == 1 and row['Elective single embryo transfer'] == 1:\n",
    "            return '1e'\n",
    "        else:\n",
    "            return row['Embryos transferred']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the e_flagging to the selected variables in the TrainSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_flagging = EFlaggingTransformer()\n",
    "df_e_flagged = e_flagging.fit_transform(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to compare\n",
    "columns_to_compare = ['Embryos transferred', 'Elective single embryo transfer']\n",
    "\n",
    "# Compare the columns before and after cleaning\n",
    "comparison_e_flagging = compare_columns(TrainSet_cleaned, df_e_flagged, columns_to_compare)\n",
    "print(comparison_e_flagging)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_flagging = EFlaggingTransformer()\n",
    "e_flagging.fit(TrainSet_cleaned)\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = e_flagging.transform(TrainSet_cleaned), e_flagging.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate the value 0 in relevant columns to indicate whether it pertains to a frozen or fresh cycle\n",
    "\n",
    "For columns related to fresh cycles ('Fresh eggs collected,' 'Total eggs mixed,' and 'Total embryos created'), if the value is 0 and it's a frozen cycle, the transformer marks it as \"0 - frozen cycle.\" Similarly, for the 'Total embryos thawed' column, if the value is 0 and it's a fresh cycle, it is marked as \"0 - fresh cycle.\" This distinction ensures that the value 0 is meaningful, reflecting its relevance to either a fresh or frozen cycle. For instance, \"0 - frozen cycle\" on the 'Fresh eggs collected' column has a different significance than a simple 0 in the context of a fresh cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeOfCycleAppender(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_update):\n",
    "        self.columns_to_update = columns_to_update\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Ensure columns have the correct data type to avoid issues\n",
    "        for column in self.columns_to_update:\n",
    "            X[column] = X[column].astype(str)\n",
    "        \n",
    "        # Apply transformation for frozen cycle\n",
    "        for column in self.columns_to_update:\n",
    "            X.loc[(X['Frozen cycle'] == 1) & (X[column] == '0'), column] = '0 - frozen cycle'\n",
    "        \n",
    "        # Apply transformation for fresh cycle\n",
    "        X['Total embryos thawed'] = X['Total embryos thawed'].astype(str)\n",
    "        X.loc[(X['Fresh cycle'] == 1) & (X['Total embryos thawed'] == '0'), 'Total embryos thawed'] = '0 - fresh cycle'\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the type_of_cycle to the selected variables in the TrainSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_update = ['Fresh eggs collected', 'Total eggs mixed', 'Total embryos created']\n",
    "\n",
    "type_of_cycle = TypeOfCycleAppender(columns_to_update=columns_to_update)\n",
    "df_type_of_cycle_appended = type_of_cycle.fit_transform(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to compare\n",
    "columns_to_compare = ['Fresh eggs collected', 'Total eggs mixed', 'Total embryos created', 'Fresh cycle', 'Frozen cycle', 'Total embryos thawed']\n",
    "\n",
    "# Compare the columns before and after cleaning\n",
    "comparison_type_of_cycle_appended = compare_columns(TrainSet_cleaned, df_type_of_cycle_appended, columns_to_compare)\n",
    "print(comparison_type_of_cycle_appended)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_update = ['Fresh eggs collected', 'Total eggs mixed', 'Total embryos created']\n",
    "type_of_cycle = TypeOfCycleAppender(columns_to_update=columns_to_update)\n",
    "type_of_cycle.fit(TrainSet_cleaned)\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = type_of_cycle.transform(TrainSet_cleaned), type_of_cycle.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check cleaning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Number of empty entries followed by the unique values and data type at each column:\\n\")\n",
    "\n",
    "for column in TrainSet_cleaned.columns:\n",
    "    # Check how many empty fields there are in each column\n",
    "    empty_fields_count = TrainSet_cleaned[column].isnull().sum()\n",
    "    # Check unique values there are in each column\n",
    "    unique_values = TrainSet_cleaned[column].unique()\n",
    "    # Check data type of each column\n",
    "    data_type = TrainSet_cleaned[column].dtype\n",
    "    \n",
    "    print (f\"- {column}: {empty_fields_count}, {unique_values}, {data_type}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop all rows with placeholder values of 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropRowsWith999(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer to drop rows with the value \"999\" in any column.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Drop rows where any column has the value \"999\"\n",
    "        X_filtered = X[(X != \"999\").all(axis=1)]\n",
    "        \n",
    "        return X_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the drop_999 to the selected variables in the TrainSet and check cleaning effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original row count: {TrainSet_cleaned.shape[0]}\")\n",
    "occurrences_before = TrainSet_cleaned.isin(['999']).sum().sum()\n",
    "print(f\"Total occurrences of '999' before cleaning: {occurrences_before}\")\n",
    "\n",
    "drop_999 = DropRowsWith999()\n",
    "df_999_dropped = drop_999.fit_transform(TrainSet_cleaned)\n",
    "\n",
    "print(f\"Cleaned row count: {df_999_dropped.shape[0]}\")\n",
    "occurrences_after = df_999_dropped.isin(['999']).sum().sum()\n",
    "\n",
    "df_999_dropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drop_999 = DropRowsWith999()\n",
    "\n",
    "drop_999.fit(TrainSet_cleaned)\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = drop_999.transform(TrainSet_cleaned), drop_999.transform(TestSet_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Number of empty entries followed by the unique values and data type at each column:\\n\")\n",
    "\n",
    "for column in TrainSet_cleaned.columns:\n",
    "    # Check how many empty fields there are in each column\n",
    "    empty_fields_count = TrainSet_cleaned[column].isnull().sum()\n",
    "    # Check unique values there are in each column\n",
    "    unique_values = TrainSet_cleaned[column].unique()\n",
    "    # Check data type of each column\n",
    "    data_type = TrainSet_cleaned[column].dtype\n",
    "    \n",
    "    print (f\"- {column}: {empty_fields_count}, {unique_values}, {data_type}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check variables with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop all ramaining rows with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import DropMissingData\n",
    "\n",
    "drop_missing_data = DropMissingData()\n",
    "df_missing_data_dropped = drop_missing_data.fit_transform(TrainSet_cleaned)\n",
    "df_missing_data_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to the Train and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drop_missing_data = DropMissingData()\n",
    "\n",
    "drop_missing_data.fit(TrainSet_cleaned)\n",
    "\n",
    "TrainSet_cleaned, TestSet_cleaned = drop_missing_data.transform(TrainSet_cleaned), drop_missing_data.transform(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that there are no more variables missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(TrainSet_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate the cleaned TrainSet and TestSet to create df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.concat([TrainSet_cleaned, TestSet_cleaned])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push cleaned data to Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create outputs/datasets/collection folder\n",
    "try:\n",
    "  os.makedirs(name='outputs/datasets/cleaned',  exist_ok=True)\n",
    "except Exception as e:\n",
    "  print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet_cleaned.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSet_cleaned.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(\"outputs/datasets/cleaned/FertilityTreatmentDataCleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
