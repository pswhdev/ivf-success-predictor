{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Perform Correlation and PPS Analysis\n",
    "* Engineer features for ML models\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* outputs/datasets/cleaned/FertilityTreatmentDataCleaned.csv\n",
    "* outputs/datasets/cleaned/TestSetCleaned.csv\n",
    "* outputs/datasets/cleaned/TrainSetCleaned.csv\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* generate a list with variables to engineer\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "Feature Engineering Transformers\n",
    "\n",
    "  * Ordinal Encoding:\n",
    "    `['Patient age at treatment',\n",
    "    'Total number of previous IVF cycles',\n",
    "    'Patient/Egg provider age',\n",
    "    'Partner/Sperm provider age',\n",
    "    'Fresh eggs collected',\n",
    "    'Total eggs mixed',\n",
    "    'Total embryos created',\n",
    "    'Embryos transferred',\n",
    "    'Total embryos thawed']`\n",
    "\n",
    "  * One Hot Encoding:\n",
    "    `['Specific treatment type',\n",
    "    'Egg source',\n",
    "    'Sperm source',\n",
    "    'Patient ethnicity',\n",
    "    'Date of embryo transfer']`\n",
    "\n",
    "  * Smart Correlation Selection:\n",
    "    `['Total embryos created',\n",
    "    'Stimulation used',\n",
    "    'Fresh cycle',\n",
    "    'Frozen cycle',\n",
    "    'Date of embryo transfer_0 - frozen']`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the working directory from its current folder to its parent folder\n",
    "* Access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the parent of the current directory the new current directory:\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"A new current directory has been set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the DataFrame from the compressed CSV file\n",
    "df = pd.read_csv('outputs/datasets/cleaned/FertilityTreatmentDataCleaned.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation and PPS Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ppscore as pps\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        mask[abs(df) < threshold] = True\n",
    "\n",
    "        fig, axes = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(\n",
    "            df,\n",
    "            annot=True,\n",
    "            xticklabels=True,\n",
    "            yticklabels=True,\n",
    "            mask=mask,\n",
    "            cmap=\"viridis\",\n",
    "            annot_kws={\"size\": font_annot},\n",
    "            ax=axes,\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "        axes.set_yticklabels(df.columns, rotation=0)\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)\n",
    "        mask[abs(df) < threshold] = True\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax = sns.heatmap(\n",
    "            df,\n",
    "            annot=True,\n",
    "            xticklabels=True,\n",
    "            yticklabels=True,\n",
    "            mask=mask,\n",
    "            cmap=\"rocket_r\",\n",
    "            annot_kws={\"size\": font_annot},\n",
    "            linewidth=0.05,\n",
    "            linecolor=\"grey\",\n",
    "        )\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def CalculateCorrAndPPS(df):\n",
    "    df_corr_spearman = df.corr(method=\"spearman\", numeric_only=True)\n",
    "    df_corr_pearson = df.corr(method=\"pearson\", numeric_only=True)\n",
    "\n",
    "    pps_matrix_raw = pps.matrix(df)\n",
    "    pps_matrix = pps_matrix_raw.filter([\"x\", \"y\", \"ppscore\"]).pivot(\n",
    "        columns=\"x\", index=\"y\", values=\"ppscore\"\n",
    "    )\n",
    "\n",
    "    pps_score_stats = (\n",
    "        pps_matrix_raw.query(\"ppscore < 1\").filter([\"ppscore\"]).describe().T\n",
    "    )\n",
    "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
    "    print(pps_score_stats.round(3))\n",
    "\n",
    "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
    "\n",
    "\n",
    "def DisplayCorrAndPPS(\n",
    "    df_corr_pearson,\n",
    "    df_corr_spearman,\n",
    "    pps_matrix,\n",
    "    CorrThreshold,\n",
    "    PPS_Threshold,\n",
    "    figsize=(20, 12),\n",
    "    font_annot=8,\n",
    "):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\n",
    "        \"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\"\n",
    "    )\n",
    "    print(\n",
    "        \"* Analyse multi-colinearity, that is, how the features are correlated among themselves\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
    "    print(\"It evaluates monotonic relationship \\n\")\n",
    "    heatmap_corr(\n",
    "        df=df_corr_spearman,\n",
    "        threshold=CorrThreshold,\n",
    "        figsize=figsize,\n",
    "        font_annot=font_annot,\n",
    "    )\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
    "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
    "    heatmap_corr(\n",
    "        df=df_corr_pearson,\n",
    "        threshold=CorrThreshold,\n",
    "        figsize=figsize,\n",
    "        font_annot=font_annot,\n",
    "    )\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
    "    print(\n",
    "        f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
    "        f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\"\n",
    "    )\n",
    "    heatmap_pps(\n",
    "        df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Correlations and Power Predictive Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCorrAndPPS(df_corr_pearson = df_corr_pearson,\n",
    "                  df_corr_spearman = df_corr_spearman, \n",
    "                  pps_matrix = pps_matrix,\n",
    "                  CorrThreshold = 0.4, PPS_Threshold =0.2,\n",
    "                  figsize=(12,10), font_annot=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tain Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_set_path = \"outputs/datasets/cleaned/TrainSetCleaned.csv\"\n",
    "TrainSet = pd.read_csv(train_set_path)\n",
    "TrainSet.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_path = 'outputs/datasets/cleaned/TestSetCleaned.csv'\n",
    "TestSet = pd.read_csv(test_set_path)\n",
    "TestSet.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "pandas_report = ProfileReport(df=TrainSet, minimal=True)\n",
    "pandas_report.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Number of empty entries followed by the unique values and data type at each column:\\n\")\n",
    "\n",
    "for column in TrainSet.columns:\n",
    "    # Check how many empty fields there are in each column\n",
    "    empty_fields_count = TrainSet[column].isnull().sum()\n",
    "    # Check unique values there are in each column\n",
    "    unique_values = TrainSet[column].unique()\n",
    "    # Check data type of each column\n",
    "    data_type = TrainSet[column].dtype\n",
    "    \n",
    "    print (f\"- {column}: {empty_fields_count}, {unique_values}, {data_type}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom transformer for Ordinal Encoding. Encodes the values in a specified order. It is saved under `src/custom_transformers.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "class OrdinalEncoderWithCategories(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categories, columns):\n",
    "        self.categories = categories\n",
    "        self.columns = columns\n",
    "        self.encoder = OrdinalEncoder()  # Initialize without categories\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensures the categories match the number of features in the data\n",
    "        if len(self.categories) != len(self.columns):\n",
    "            raise ValueError(\n",
    "                \"The number of category lists must match the number of features.\"\n",
    "            )\n",
    "        # Set categories during fit\n",
    "        self.encoder.set_params(categories=self.categories)\n",
    "        self.encoder.fit(X[self.columns])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        transformed = self.encoder.transform(X[self.columns])\n",
    "        # Return a DataFrame with the original column names\n",
    "        X[self.columns] = pd.DataFrame(transformed, columns=self.columns, index=X.index)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from feature_engine import transformation as vt\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def FeatureEngineeringAnalysis(df, analysis_type=None):\n",
    "    \"\"\"\n",
    "    - used for quick feature engineering on numerical and categorical variables\n",
    "    to decide which transformation can better transform the distribution shape\n",
    "    - Once transformed, use a reporting tool, like ydata-profiling, to evaluate distributions\n",
    "    \"\"\"\n",
    "    check_missing_values(df)\n",
    "    allowed_types = [\n",
    "        \"numerical\",\n",
    "        \"ordinal_encoder\",\n",
    "        \"outlier_winsorizer\",\n",
    "        \"one_hot_encoder\",\n",
    "    ]\n",
    "    check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
    "    list_column_transformers = define_list_column_transformers(analysis_type)\n",
    "\n",
    "    # Loop in each variable and engineer the data according to the analysis type\n",
    "    df_feat_eng = pd.DataFrame([])\n",
    "    for column in df.columns:\n",
    "        # create additional columns (column_method) to apply the methods\n",
    "        df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
    "        for method in list_column_transformers:\n",
    "            df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
    "\n",
    "        # Apply transformers in respective column_transformers\n",
    "        df_feat_eng, list_applied_transformers = apply_transformers(\n",
    "            analysis_type, df_feat_eng, column\n",
    "        )\n",
    "\n",
    "        # For each variable, assess how the transformations perform\n",
    "        transformer_evaluation(\n",
    "            column, list_applied_transformers, analysis_type, df_feat_eng\n",
    "        )\n",
    "\n",
    "    return df_feat_eng\n",
    "\n",
    "\n",
    "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
    "    \"\"\"Check analysis type\"\"\"\n",
    "    if analysis_type is None:\n",
    "        raise SystemExit(\n",
    "            f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\"\n",
    "        )\n",
    "    if analysis_type not in allowed_types:\n",
    "        raise SystemExit(\n",
    "            f\"analysis_type argument should be one of these options: {allowed_types}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def check_missing_values(df):\n",
    "    if df.isna().sum().sum() != 0:\n",
    "        raise SystemExit(\n",
    "            f\"There is a missing value in your dataset. Please handle that before getting into feature engineering.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def define_list_column_transformers(analysis_type):\n",
    "    \"\"\"Set suffix columns according to analysis_type\"\"\"\n",
    "    if analysis_type == \"numerical\":\n",
    "        list_column_transformers = [\n",
    "            \"log_e\",\n",
    "            \"log_10\",\n",
    "            \"reciprocal\",\n",
    "            \"power\",\n",
    "            \"box_cox\",\n",
    "            \"yeo_johnson\",\n",
    "        ]\n",
    "\n",
    "    elif analysis_type == \"ordinal_encoder\":\n",
    "        list_column_transformers = [\"ordinal_encoder\"]\n",
    "\n",
    "    elif analysis_type == \"outlier_winsorizer\":\n",
    "        list_column_transformers = [\"iqr\"]\n",
    "\n",
    "    elif analysis_type == \"one_hot_encoder\":\n",
    "        list_column_transformers = [\"one_hot_encoder\"]\n",
    "\n",
    "    return list_column_transformers\n",
    "\n",
    "\n",
    "def apply_transformers(analysis_type, df_feat_eng, column):\n",
    "    for col in df_feat_eng.select_dtypes(include=\"category\").columns:\n",
    "        df_feat_eng[col] = df_feat_eng[col].astype(\"object\")\n",
    "\n",
    "    if analysis_type == \"numerical\":\n",
    "        df_feat_eng, list_applied_transformers = FeatEngineering_Numerical(\n",
    "            df_feat_eng, column\n",
    "        )\n",
    "\n",
    "    elif analysis_type == \"outlier_winsorizer\":\n",
    "        df_feat_eng, list_applied_transformers = FeatEngineering_OutlierWinsorizer(\n",
    "            df_feat_eng, column\n",
    "        )\n",
    "\n",
    "    elif analysis_type == \"ordinal_encoder\":\n",
    "        df_feat_eng, list_applied_transformers = FeatEngineering_CustomOrdinalEncoder(\n",
    "            df_feat_eng, column\n",
    "        )\n",
    "\n",
    "    elif analysis_type == \"one_hot_encoder\":\n",
    "        df_feat_eng, list_applied_transformers = FeatEngineering_OneHotEncoder(\n",
    "            df_feat_eng, column\n",
    "        )\n",
    "\n",
    "    return df_feat_eng, list_applied_transformers\n",
    "\n",
    "\n",
    "def transformer_evaluation(\n",
    "    column, list_applied_transformers, analysis_type, df_feat_eng\n",
    "):\n",
    "    # For each variable, assess how the transformations perform\n",
    "    print(f\"* Variable Analyzed: {column}\")\n",
    "    print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
    "\n",
    "    if analysis_type == \"one_hot_encoder\":\n",
    "        # Show the first few rows and the shape of the DataFrame after encoding\n",
    "        print(f\"Transformed DataFrame shape: {df_feat_eng.shape}\")\n",
    "        print(\"Columns added during One Hot Encoding:\\n\")\n",
    "        print([col for col in df_feat_eng.columns if column in col])  # Display columns created by One-Hot Encoding\n",
    "        print(df_feat_eng.head())  # Display the first few rows to inspect the transformations\n",
    "\n",
    "    else:\n",
    "        for col in [column] + list_applied_transformers:\n",
    "            if analysis_type != \"ordinal_encoder\":\n",
    "                DiagnosticPlots_Numerical(df_feat_eng, col)\n",
    "            else:\n",
    "                if col == column:\n",
    "                    DiagnosticPlots_Categories(df_feat_eng, col)\n",
    "                else:\n",
    "                    DiagnosticPlots_Numerical(df_feat_eng, col)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.countplot(\n",
    "        data=df_feat_eng,\n",
    "        x=col,\n",
    "        palette=[\"#432371\"],\n",
    "        order=df_feat_eng[col].value_counts().index,\n",
    "    )\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.suptitle(f\"{col}\", fontsize=30, y=1.05)\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "def DiagnosticPlots_Numerical(df, variable):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    sns.histplot(data=df, x=variable, kde=True, element=\"step\", ax=axes[0])\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
    "    sns.boxplot(x=df[variable], ax=axes[2])\n",
    "\n",
    "    axes[0].set_title(\"Histogram\")\n",
    "    axes[1].set_title(\"QQ Plot\")\n",
    "    axes[2].set_title(\"Boxplot\")\n",
    "    fig.suptitle(f\"{variable}\", fontsize=30, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def map_categories_to_order(df, column, category_order):\n",
    "    \"\"\" Map categories to their respective order based on predefined order \"\"\"\n",
    "    category_mapping = {category: idx for idx, category in enumerate(category_order)}\n",
    "    df[f\"{column}_ordinal_encoder\"] = df[column].map(category_mapping)\n",
    "    return df\n",
    "\n",
    "def FeatEngineering_CustomOrdinalEncoder(df_feat_eng, column):\n",
    "    list_methods_worked = []\n",
    "\n",
    "    try:\n",
    "        # Find the index of the current column in the list of columns\n",
    "        column_index = columns.index(column)\n",
    "        \n",
    "        # Use the corresponding categories based on the column index\n",
    "        custom_encoder = OrdinalEncoderWithCategories(\n",
    "            categories=[categories[column_index]],  # Match category to the column\n",
    "            columns=[column]\n",
    "        )\n",
    "\n",
    "        # Apply the custom transformer\n",
    "        df_feat_eng = custom_encoder.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding {column} with custom encoder: {e}\")\n",
    "        if f\"{column}_ordinal_encoder\" in df_feat_eng.columns:\n",
    "            df_feat_eng.drop([f\"{column}_ordinal_encoder\"], axis=1, inplace=True)\n",
    "\n",
    "    return df_feat_eng, list_methods_worked\n",
    "\n",
    "\n",
    "def FeatEngineering_OutlierWinsorizer(df_feat_eng, column):\n",
    "    list_methods_worked = []\n",
    "\n",
    "    # Winsorizer iqr\n",
    "    try:\n",
    "        disc = Winsorizer(\n",
    "            capping_method=\"iqr\", tail=\"both\", fold=1.5, variables=[f\"{column}_iqr\"]\n",
    "        )\n",
    "        df_feat_eng = disc.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_iqr\")\n",
    "    except Exception:\n",
    "        df_feat_eng.drop([f\"{column}_iqr\"], axis=1, inplace=True)\n",
    "\n",
    "    return df_feat_eng, list_methods_worked\n",
    "\n",
    "\n",
    "def FeatEngineering_Numerical(df_feat_eng, column):\n",
    "    list_methods_worked = []\n",
    "\n",
    "    # LogTransformer base e\n",
    "    try:\n",
    "        lt = vt.LogTransformer(variables=[f\"{column}_log_e\"])\n",
    "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_log_e\")\n",
    "    except Exception:\n",
    "        df_feat_eng.drop([f\"{column}_log_e\"], axis=1, inplace=True)\n",
    "\n",
    "    # LogTransformer base 10\n",
    "    try:\n",
    "        lt = vt.LogTransformer(variables=[f\"{column}_log_10\"], base=\"10\")\n",
    "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_log_10\")\n",
    "    except Exception:\n",
    "        df_feat_eng.drop([f\"{column}_log_10\"], axis=1, inplace=True)\n",
    "\n",
    "    # ReciprocalTransformer\n",
    "    try:\n",
    "        rt = vt.ReciprocalTransformer(variables=[f\"{column}_reciprocal\"])\n",
    "        df_feat_eng = rt.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_reciprocal\")\n",
    "    except Exception:\n",
    "        df_feat_eng.drop([f\"{column}_reciprocal\"], axis=1, inplace=True)\n",
    "\n",
    "    # PowerTransformer\n",
    "    try:\n",
    "        pt = vt.PowerTransformer(variables=[f\"{column}_power\"])\n",
    "        df_feat_eng = pt.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_power\")\n",
    "    except Exception:\n",
    "        df_feat_eng.drop([f\"{column}_power\"], axis=1, inplace=True)\n",
    "\n",
    "    # BoxCoxTransformer\n",
    "    try:\n",
    "        bct = vt.BoxCoxTransformer(variables=[f\"{column}_box_cox\"])\n",
    "        df_feat_eng = bct.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_box_cox\")\n",
    "    except Exception:\n",
    "        df_feat_eng.drop([f\"{column}_box_cox\"], axis=1, inplace=True)\n",
    "\n",
    "    # YeoJohnsonTransformer\n",
    "    try:\n",
    "        yjt = vt.YeoJohnsonTransformer(variables=[f\"{column}_yeo_johnson\"])\n",
    "        df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
    "        list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
    "    except Exception:\n",
    "        df_feat_eng.drop([f\"{column}_yeo_johnson\"], axis=1, inplace=True)\n",
    "\n",
    "    return df_feat_eng, list_methods_worked\n",
    "\n",
    "\n",
    "def FeatEngineering_OneHotEncoder(df_feat_eng, column):\n",
    "    list_methods_worked = []\n",
    "    try:\n",
    "        # Initialize the OneHotEncoder from feature_engine\n",
    "        encoder = OneHotEncoder(drop_last=True, variables=[column])\n",
    "\n",
    "        # Fit and transform the dataframe\n",
    "        df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
    "\n",
    "        # Record the applied transformations\n",
    "        encoded_columns = df_feat_eng.columns[\n",
    "            df_feat_eng.columns.str.startswith(column)\n",
    "        ]\n",
    "        list_methods_worked.extend(encoded_columns)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with column {column}: {e}\")\n",
    "        if any(f\"{column}_\" in col for col in df_feat_eng.columns):\n",
    "            columns_to_drop = [\n",
    "                col for col in df_feat_eng.columns if f\"{column}_\" in col\n",
    "            ]\n",
    "            df_feat_eng.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    return df_feat_eng, list_methods_worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Categorical Encoding (Ordinal Encoder and OneHotEncoder)\n",
    "* Smart Correlation Selection (SmartCorrelatedSelection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal Encoder\n",
    "\n",
    "Categorical Encoding - Ordinal: replaces categories with ordinal numbers. For features where the order of categories is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_columns = [\n",
    "        'Patient age at treatment',\n",
    "        'Total number of previous IVF cycles',\n",
    "        'Patient/Egg provider age',\n",
    "        'Partner/Sperm provider age',\n",
    "        'Fresh eggs collected',\n",
    "        'Total eggs mixed',\n",
    "        'Total embryos created',\n",
    "        'Embryos transferred',\n",
    "        'Total embryos thawed'\n",
    "        ]\n",
    "\n",
    "ordinal_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the categories for each column in the same order as ordinal_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper order for the values to be encoded\n",
    "categories = [\n",
    "    ['18-34', '35-37', '38-39', '40-42', '43-44', '45-50'],  # Patient age at treatment\n",
    "    ['0', '1', '2', '3', '4', '5', '>5'],  # Total number of previous IVF cycles\n",
    "    ['18-34', '35-37', '38-39', '40-42', '43-44', '45-50'],  # Patient/Egg provider age\n",
    "    ['18-34', '35-37', '38-39', '40-42', '43-44', '45-50', '51-55', '56-60', '>60'],  # Partner/Sperm provider age\n",
    "    ['0', '0 - frozen cycle', '1-5', '6-10', '11-15', '16-20', '21-25', '26-30', '31-35', '36-40', '>40'],  # Fresh eggs collected\n",
    "    ['0', '0 - frozen cycle', '1-5', '6-10', '11-15', '16-20', '21-25', '26-30', '31-35', '36-40', '>40'],  # Total eggs mixed\n",
    "    ['0', '0 - frozen cycle', '1-5', '6-10', '11-15', '16-20', '21-25', '26-30', '>30'],  # Total embryos created\n",
    "    ['0', '1', '1e', '2', '3'],  # Embryos transferred\n",
    "    ['0 - fresh cycle', '0 - frozen cycle', '1-5', '6-10', '>10']  # Total embryos thawed\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that categories match the order and number of ordinal columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(categories) != len(ordinal_columns):\n",
    "    raise ValueError(\"The number of categories must match the number of ordinal columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a separate DataFrame with the selected variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_oe = TrainSet[ordinal_columns].copy()\n",
    "df_eng_oe.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the selected transformation to the Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoderWithCategories(categories=categories, columns=ordinal_columns)\n",
    "TrainSet = ordinal_encoder.fit_transform(TrainSet)\n",
    "TestSet = ordinal_encoder.transform(TestSet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the result of the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TrainSet.head())\n",
    "for column in ordinal_columns:\n",
    "    print(f\"\\nUnique values in transformed '{column}' column (TrainSet): {TrainSet[column].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### OneHotEncoder:\n",
    "\n",
    "One hot encoder was used for variables with values without inherent order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_eng_ohe=[\n",
    "    \"Specific treatment type\",\n",
    "    \"Egg source\",\n",
    "    \"Sperm source\",\n",
    "    \"Patient ethnicity\",\n",
    "    \"Date of embryo transfer\"\n",
    "]\n",
    "\n",
    "variables_eng_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a separate DataFrame with the selected variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_ohe = TrainSet[variables_eng_ohe].copy()\n",
    "df_eng_ohe.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the selected transformation to the Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_ohe = FeatureEngineeringAnalysis(df=df_eng_ohe, analysis_type='one_hot_encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to the Train ans Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_encoder = OneHotEncoder(top_categories=None, drop_last=True, variables = variables_eng_ohe)\n",
    "TrainSet = ohe_encoder.fit_transform(TrainSet)\n",
    "TestSet = ohe_encoder.transform(TestSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SmartCorrelatedSelection Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To be applied in all variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe to apply the transformer on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_smart_corr_sel = TrainSet.copy()\n",
    "df_eng_smart_corr_sel.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The threshold of the smart correation was kept at 0.9 to keep as much important infomrartion as possible due to the low predictive score of the features from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.9, selection_method=\"variance\")\n",
    "\n",
    "corr_sel.fit_transform(df_eng_smart_corr_sel)\n",
    "corr_sel.correlated_feature_sets_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what freatures should be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sel.features_to_drop_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations needed for feature engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Feature Engineering Transformers\n",
    "\n",
    "  * Ordinal Encoding:\n",
    "    `['Patient age at treatment',\n",
    "    'Total number of previous IVF cycles',\n",
    "    'Patient/Egg provider age',\n",
    "    'Partner/Sperm provider age',\n",
    "    'Fresh eggs collected',\n",
    "    'Total eggs mixed',\n",
    "    'Total embryos created',\n",
    "    'Embryos transferred',\n",
    "    'Total embryos thawed']`\n",
    "\n",
    "  * One Hot Encoding:\n",
    "    `['Specific treatment type',\n",
    "    'Egg source',\n",
    "    'Sperm source',\n",
    "    'Patient ethnicity',\n",
    "    'Date of embryo transfer']`\n",
    "\n",
    "  * Smart Correlation Selection:\n",
    "    `['Total embryos created',\n",
    "    'Stimulation used',\n",
    "    'Fresh cycle',\n",
    "    'Frozen cycle',\n",
    "    'Date of embryo transfer_0 - frozen']`\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
