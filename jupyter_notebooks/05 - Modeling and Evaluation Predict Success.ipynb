{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "*   Fit and evaluate a classification model to predict if a treatment will be successful or not.\n",
    "\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* outputs/datasets/collection/FertilityTreatmentData.csv.gz\n",
    "* Instructions from the notebooks 02 and 04 on which variables to use for data cleaning and feature engineering.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Train set (features and target)\n",
    "* Test set (features and target)\n",
    "* Data cleaning and Feature Engineering pipeline\n",
    "* Modeling pipeline\n",
    "* Feature importance plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the working directory from its current folder to its parent folder\n",
    "* Access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the parent of the current directory the new current directory:\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"A new current directory has been set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Open dataset\n",
    "df = pd.read_csv(\"outputs/datasets/collection/FertilityTreatmentData.csv.gz\")\n",
    "        \n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ML Pipeline with all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML pipeline for Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class FilterIVFTreatments(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.query(\n",
    "            \"`Main reason for producing embroys storing eggs` == 'Treatment - IVF'\"\n",
    "        )\n",
    "\n",
    "class DropErroneousEntries(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.drop(\n",
    "            X[(X[\"Live birth occurrence\"] == 1) & (X[\"Embryos transferred\"] == 0)].index\n",
    "        )\n",
    "\n",
    "class ConvertToNumeric(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in self.columns:\n",
    "            # Replace '>3' with 4\n",
    "            X[col] = X[col].replace(\">3\", 4)\n",
    "            # Convert to numeric\n",
    "            X[col] = pd.to_numeric(X[col])\n",
    "        return X\n",
    "\n",
    "\n",
    "class ConvertToIntegers(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in self.columns:\n",
    "            # Replace '>3' with 4 and convert to int\n",
    "            X[col] = X[col].replace(\">3\", 4).astype(float).astype(int)\n",
    "        return X\n",
    "\n",
    "\n",
    "class FillSpermSource(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"Sperm source\"] = X.apply(self._fill_sperm_source, axis=1)\n",
    "        return X\n",
    "\n",
    "    def _fill_sperm_source(self, row):\n",
    "        if pd.isna(row[\"Sperm source\"]):\n",
    "            if not pd.isna(row[\"Sperm donor age at registration\"]):\n",
    "                return \"Donor\"\n",
    "            else:\n",
    "                return \"Partner\"\n",
    "        return row[\"Sperm source\"]\n",
    "\n",
    "\n",
    "# Convert float values to integers and handle NaN values\n",
    "class ConvertToIntAndReplace999(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Fill NaN with -1 and convert to int\n",
    "        X[\"Date of embryo transfer\"] = (\n",
    "            X[\"Date of embryo transfer\"].fillna(-1).astype(int)\n",
    "        )\n",
    "        # Replace 999 with 0\n",
    "        X[\"Date of embryo transfer\"] = X[\"Date of embryo transfer\"].replace(999, 0)\n",
    "        return X\n",
    "\n",
    "\n",
    "# Replace missing values based on the \"Embryos transferred\" column\n",
    "class ReplaceMissingValues(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"Date of embryo transfer\"] = X.apply(self._replace_missing, axis=1)\n",
    "        return X\n",
    "\n",
    "    def _replace_missing(self, row):\n",
    "        value = row[\"Date of embryo transfer\"]\n",
    "        if value == -1 and row[\"Embryos transferred\"] == 0:\n",
    "            return \"NT\"\n",
    "        elif value == -1:\n",
    "            return \"Missing\"\n",
    "        return value\n",
    "\n",
    "\n",
    "# Append strings based on the \"Fresh cycle\" and \"Frozen cycle\" values\n",
    "class AppendCycleType(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"Date of embryo transfer\"] = X.apply(self._append_cycle_type, axis=1)\n",
    "        return X\n",
    "\n",
    "    def _append_cycle_type(self, row):\n",
    "        value = row[\"Date of embryo transfer\"]\n",
    "        if value not in [\"NT\", \"Missing\"]:\n",
    "            if row[\"Fresh cycle\"] == 1:\n",
    "                value = f\"{value} - fresh\"\n",
    "            elif row[\"Frozen cycle\"] == 1:\n",
    "                value = f\"{value} - frozen\"\n",
    "            else:\n",
    "                value = f\"{value} - Mixed fresh/frozen\"\n",
    "        return value\n",
    "\n",
    "\n",
    "class MicroInjectedEmbryos(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Embryos transferred from eggs micro-injected imputation\n",
    "        missing_micro_injected = X[\n",
    "            \"Embryos transferred from eggs micro-injected\"\n",
    "        ].isna()\n",
    "        ICSI = X[\"Specific treatment type\"].str.contains(\"ICSI\")\n",
    "        # Only replace missing values\n",
    "        X.loc[\n",
    "            missing_micro_injected & ICSI,\n",
    "            \"Embryos transferred from eggs micro-injected\",\n",
    "        ] = X.loc[missing_micro_injected & ICSI, \"Embryos transferred\"]\n",
    "        X.loc[\n",
    "            missing_micro_injected & ~ICSI,\n",
    "            \"Embryos transferred from eggs micro-injected\",\n",
    "        ] = 0\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "class DonorAgeImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Mapping from donor age ranges to patient/partner age ranges\n",
    "        self.egg_age_map = {\n",
    "            \"Between 21 and 25\": \"18-34\",\n",
    "            \"Between 26 and 30\": \"18-34\",\n",
    "            \"Between 31 and 35\": \"18-34\",\n",
    "            \">35\": \"38-39\",\n",
    "            \"<= 20\": \"18-34\",\n",
    "        }\n",
    "        self.sperm_age_map = {\n",
    "            \"Between 21 and 25\": \"18-34\",\n",
    "            \"Between 26 and 30\": \"18-34\",\n",
    "            \"Between 31 and 35\": \"18-34\",\n",
    "            \"Between 36 and 40\": \"38-39\",\n",
    "            \"Between 41 and 45\": \"43-44\",\n",
    "            \">45\": \"45-50\",\n",
    "            \"<= 20\": \"18-34\",\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Egg donor age imputation\n",
    "        X[\"Egg donor age at registration\"] = X[\"Egg donor age at registration\"].map(\n",
    "            self.egg_age_map\n",
    "        )\n",
    "        missing_egg_age = (X[\"Egg donor age at registration\"].isna()) & (\n",
    "            X[\"Egg source\"] == \"Patient\"\n",
    "        )\n",
    "        X.loc[missing_egg_age, \"Egg donor age at registration\"] = X.loc[\n",
    "            missing_egg_age, \"Patient age at treatment\"\n",
    "        ]\n",
    "        X.rename(\n",
    "            columns={\"Egg donor age at registration\": \"Patient/Egg provider age\"},\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # Sperm donor age imputation\n",
    "        X[\"Sperm donor age at registration\"] = X[\"Sperm donor age at registration\"].map(\n",
    "            self.sperm_age_map\n",
    "        )\n",
    "        missing_sperm_age = (X[\"Sperm donor age at registration\"].isna()) & (\n",
    "            X[\"Sperm source\"] == \"Partner\"\n",
    "        )\n",
    "        X.loc[missing_sperm_age, \"Sperm donor age at registration\"] = X.loc[\n",
    "            missing_sperm_age, \"Partner age\"\n",
    "        ]\n",
    "        X.rename(\n",
    "            columns={\"Sperm donor age at registration\": \"Partner/Sperm provider age\"},\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # Ensure no duplicate columns\n",
    "        if X.columns.duplicated().any():\n",
    "            raise ValueError(\"Duplicate column names found after transformation\")\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "class FloatToIntTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.float_vars = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Identify float columns\n",
    "        self.float_vars = X.select_dtypes(include=\"float\").columns.tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for var in self.float_vars:\n",
    "            X[var] = X[var].astype(int)\n",
    "        return X\n",
    "\n",
    "\n",
    "class EFlaggingTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        X[\"Embryos transferred\"] = X.apply(self.append_e, axis=1)\n",
    "        return X\n",
    "\n",
    "    def append_e(self, row):\n",
    "        if (\n",
    "            row[\"Embryos transferred\"] == 1\n",
    "            and row[\"Elective single embryo transfer\"] == 1\n",
    "        ):\n",
    "            return \"1e\"\n",
    "        else:\n",
    "            return row[\"Embryos transferred\"]\n",
    "\n",
    "\n",
    "class TypeOfCycleAppender(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_update):\n",
    "        self.columns_to_update = columns_to_update\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Ensure columns have the correct data type to avoid issues\n",
    "        for column in self.columns_to_update:\n",
    "            X[column] = X[column].astype(str)\n",
    "\n",
    "        # Apply transformation for frozen cycle\n",
    "        for column in self.columns_to_update:\n",
    "            X.loc[(X[\"Frozen cycle\"] == 1) & (X[column] == \"0\"), column] = (\n",
    "                \"0 - frozen cycle\"\n",
    "            )\n",
    "\n",
    "        # Apply transformation for fresh cycle\n",
    "        X[\"Total embryos thawed\"] = X[\"Total embryos thawed\"].astype(str)\n",
    "        X.loc[\n",
    "            (X[\"Fresh cycle\"] == 1) & (X[\"Total embryos thawed\"] == \"0\"),\n",
    "            \"Total embryos thawed\",\n",
    "        ] = \"0 - fresh cycle\"\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.selection import DropFeatures, SmartCorrelatedSelection\n",
    "from feature_engine.imputation import ArbitraryNumberImputer, DropMissingData\n",
    "from feature_engine.encoding import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = [\n",
    "    \"Main reason for producing embroys storing eggs\",\n",
    "    \"Type of treatment - IVF or DI\",\n",
    "    \"Donated embryo\",\n",
    "    \"Eggs thawed (0/1)\",\n",
    "    \"Year of treatment\",\n",
    "    \"Number of live births\",\n",
    "    \"Embryos stored for use by patient\",\n",
    "    \"Fresh eggs stored (0/1)\",\n",
    "    \"Heart three birth congenital abnormalities\",\n",
    "    \"Heart two birth congenital abnormalities\",\n",
    "    \"Heart three delivery date\",\n",
    "    \"Heart three sex\",\n",
    "    \"Heart three birth weight\",\n",
    "    \"Heart three weeks gestation\",\n",
    "    \"Heart three birth outcome\",\n",
    "    \"Heart one birth congenital abnormalities\",\n",
    "    \"Heart two birth weight\",\n",
    "    \"Heart two delivery date\",\n",
    "    \"Heart two sex\",\n",
    "    \"Heart two weeks gestation\",\n",
    "    \"Heart two birth outcome\",\n",
    "    \"Heart one birth weight\",\n",
    "    \"Heart one weeks gestation\",\n",
    "    \"Heart one delivery date\",\n",
    "    \"Heart one sex\",\n",
    "    \"Heart one birth outcome\",\n",
    "    \"Number of foetal sacs with fetal pulsation\",\n",
    "    \"Early outcome\",\n",
    "    \"Partner Type\",\n",
    "]\n",
    "\n",
    "# Columns to be updated with the type of cycle\n",
    "columns_to_update = [\n",
    "    \"Fresh eggs collected\",\n",
    "    \"Total eggs mixed\",\n",
    "    \"Total embryos created\",\n",
    "]\n",
    "\n",
    "def PipelineDataCleaningAndFeatureEngineering():\n",
    "    pipeline_base = Pipeline(\n",
    "        [\n",
    "            # Data Cleaning Steps\n",
    "            (\"filter_ivf\", FilterIVFTreatments()),\n",
    "            (\"drop_erroneous\", DropErroneousEntries()),\n",
    "            (\"drop_columns\", DropFeatures(features_to_drop=columns_to_drop)),\n",
    "            (\n",
    "                \"convert_to_numeric\",\n",
    "                ConvertToNumeric(\n",
    "                    columns=[\n",
    "                        \"Total number of previous pregnancies - IVF and DI\",\n",
    "                        \"Total number of previous live births - IVF or DI\",\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"zeros_imputer\",\n",
    "                ArbitraryNumberImputer(\n",
    "                    arbitrary_number=0,\n",
    "                    variables=[\n",
    "                        \"Total number of previous pregnancies - IVF and DI\",\n",
    "                        \"Total number of previous live births - IVF or DI\",\n",
    "                    ],\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"convert_to_int\",\n",
    "                ConvertToIntegers(\n",
    "                    columns=[\n",
    "                        \"Total number of previous pregnancies - IVF and DI\",\n",
    "                        \"Total number of previous live births - IVF or DI\",\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            (\"fill_sperm_source\", FillSpermSource()),  \n",
    "            (\"dot_to_int_999\", ConvertToIntAndReplace999()),  \n",
    "            (\"replace_missing_values\", ReplaceMissingValues()),  \n",
    "            (\"append_cycle_type\", AppendCycleType()),  \n",
    "            (\"micro_injected\", MicroInjectedEmbryos()),  \n",
    "            (\"donor_age\", DonorAgeImputer()),  \n",
    "            (\"float_to_int\", FloatToIntTransformer()),  \n",
    "            (\"e_flagging\", EFlaggingTransformer()),  \n",
    "            (\"type_of_cycle\", TypeOfCycleAppender(columns_to_update=columns_to_update)),\n",
    "            (\"drop_missing_data\", DropMissingData()),\n",
    "\n",
    "            # Feature Engineering Steps\n",
    "            (\n",
    "                \"ordinal_encoding\",\n",
    "                OrdinalEncoder(\n",
    "                    encoding_method='arbitrary',\n",
    "                    variables=[\n",
    "                        \"Patient age at treatment\",\n",
    "                        \"Partner/Sperm provider age\",\n",
    "                        \"Patient/Egg provider age\",\n",
    "                        \"Total number of previous IVF cycles\",\n",
    "                        \"Total number of previous DI cycles\",\n",
    "                        \"Fresh eggs collected\",\n",
    "                        \"Total eggs mixed\",\n",
    "                        \"Total embryos created\",\n",
    "                        \"Embryos transferred\",\n",
    "                        \"Total embryos thawed\",\n",
    "                        \"Date of embryo transfer\",\n",
    "                        \"Partner age\"\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"one_hot_encoding\",\n",
    "                OneHotEncoder(\n",
    "                    variables=[\n",
    "                        \"Specific treatment type\",\n",
    "                        \"Egg source\",\n",
    "                        \"Sperm source\",\n",
    "                        \"Patient ethnicity\",\n",
    "                        \"Partner ethnicity\",\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            (\"smart_correlation\", SmartCorrelatedSelection()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipeline_base\n",
    "\n",
    "PipelineDataCleaningAndFeatureEngineering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Pipeline for Modelling and Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# ML algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def PipelineClf(model):\n",
    "    pipeline_base = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"feat_selection\", SelectFromModel(model)),\n",
    "            (\"model\", model),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Class for Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs=-1, verbose=1, scoring=None, refit=\"f1\"):\n",
    "        for key in self.keys:\n",
    "            print(\n",
    "                f\"\\nRunning GridSearchCV for {key} with {len(self.params[key])} parameter combinations.\\n\"\n",
    "            )\n",
    "\n",
    "            model = PipelineClf(self.models[key])\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(\n",
    "                model,\n",
    "                params,\n",
    "                cv=cv,\n",
    "                n_jobs=n_jobs,\n",
    "                verbose=verbose,\n",
    "                scoring=scoring,\n",
    "                refit=refit,\n",
    "            )\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "            \n",
    "    def score_summary(self, sort_by=\"f1\", scoring=None):\n",
    "        if scoring is None:\n",
    "            raise ValueError(\"Scoring dictionary must be provided\")\n",
    "        \n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_[\"params\"]\n",
    "            mean_scores = {\n",
    "                metric: self.grid_searches[k].cv_results_[f\"mean_test_{metric}\"]\n",
    "                for metric in scoring.keys()\n",
    "            }\n",
    "            \n",
    "            for i in range(len(params)):\n",
    "                row_data = {\"estimator\": k, **params[i]}\n",
    "                row_data.update({f\"mean_{metric}\": mean_scores[metric][i] for metric in mean_scores})\n",
    "                rows.append(pd.Series(row_data))\n",
    "        \n",
    "        df = pd.DataFrame(rows).sort_values([f\"mean_{sort_by}\"], ascending=False)\n",
    "        return df, self.grid_searches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    # Do not drop the target column here because it is needed for the pipeline\n",
    "    df,\n",
    "    df[\"Live birth occurrence\"],\n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Target Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the data cleaning and feature engineering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data_cleaning_feat_eng = PipelineDataCleaningAndFeatureEngineering()\n",
    "X_train = pipeline_data_cleaning_feat_eng.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realign y_train indices with the transformed X_train to keep only the rows present on the dataset after the cleaning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.loc[X_train.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the pipeline to the test set and realign indices on y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pipeline_data_cleaning_feat_eng.transform(X_test)\n",
    "y_test = y_test.loc[X_test.index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the target column from the processed X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop([\"Live birth occurrence\"], axis=1)\n",
    "X_test = X_test.drop([\"Live birth occurrence\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Train Set Target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(y_train.value_counts())\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "y_train.value_counts().plot(kind=\"bar\", title=\"Train Set Target Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Use SMOTE (Synthetic Minority Oversampling TEchnique) to balance Train Set target -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the imbalance problem, the strategy described by Peng et al. (2022) in their study \"Predicting live birth in in vitro fertilization: A machine learning-based clinical prediction model using 57,558 cycles\" (Frontiers in Endocrinology, 13, 838087, https://doi.org/10.3389/fendo.2022.838087) was employed.\n",
    "\n",
    "Given that the imbalance ratio in this study was moderate, a strategy was implemented to address the imbalance without the drawbacks of traditional methods, such as information loss from undersampling or overfitting from oversampling.\n",
    "\n",
    "The dataset will be divided into 3 sub-datasets, each maintaining a balanced ratio of positive to negative samples. Each sub-dataset will include all positive samples and one-third of the negative samples.\n",
    "\n",
    "Decision Tree (DT) and Linear Discriminant (LD) models will be used to pre-train these sub-datasets, and key metrics like precision, recall, and F1 score will be evaluated.\n",
    "\n",
    "The sub-dataset with the best performance metrics will then be selected and used for final model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Dataset into Sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate positive and negative samples\n",
    "X_positive = X_train[y_train == 1]\n",
    "X_negative = X_train[y_train == 0]\n",
    "\n",
    "y_positive = y_train[y_train == 1]\n",
    "y_negative = y_train[y_train == 0]\n",
    "\n",
    "# Number of positive samples\n",
    "num_positive = len(X_positive)\n",
    "\n",
    "# Split negative samples into three equal parts\n",
    "X_negative_splits = np.array_split(X_negative, 3)\n",
    "y_negative_splits = np.array_split(y_negative, 3)\n",
    "\n",
    "sub_X = []\n",
    "sub_y = []\n",
    "\n",
    "for i in range(3):\n",
    "    # Start with one-third of the negative samples\n",
    "    X_negative_subset = X_negative_splits[i]\n",
    "    y_negative_subset = y_negative_splits[i]\n",
    "    \n",
    "    # If more negative samples are needed to match the number of positive samples,\n",
    "    # sample from the remaining negative samples across the other two splits\n",
    "    if len(X_negative_subset) < num_positive:\n",
    "        additional_X_negatives = pd.concat(X_negative_splits[:i] + X_negative_splits[i+1:])\n",
    "        additional_y_negatives = pd.concat(y_negative_splits[:i] + y_negative_splits[i+1:])\n",
    "        \n",
    "        extra_X = resample(additional_X_negatives, n_samples=num_positive - len(X_negative_subset), replace=False, random_state=i)\n",
    "        extra_y = resample(additional_y_negatives, n_samples=num_positive - len(y_negative_subset), replace=False, random_state=i)\n",
    "        \n",
    "        # Combine the subset with the extra samples\n",
    "        X_negative_subset = pd.concat([X_negative_subset, extra_X])\n",
    "        y_negative_subset = pd.concat([y_negative_subset, extra_y])\n",
    "    \n",
    "    # Combine with all positive samples\n",
    "    X_sub = pd.concat([X_positive, X_negative_subset])\n",
    "    y_sub = pd.concat([y_positive, y_negative_subset])\n",
    "    \n",
    "    # Shuffle the sub-dataset\n",
    "    X_sub, y_sub = resample(X_sub, y_sub, random_state=i)\n",
    "    \n",
    "    # Append to the list of sub-datasets\n",
    "    sub_X.append(X_sub)\n",
    "    sub_y.append(y_sub)\n",
    "\n",
    "# After creating the sub-datasets\n",
    "for i in range(3):\n",
    "    X_sub = sub_X[i]\n",
    "    y_sub = sub_y[i]\n",
    "    \n",
    "    # Count positive and negative samples\n",
    "    num_positive = y_sub.value_counts()[1]\n",
    "    num_negative = y_sub.value_counts()[0]\n",
    "    \n",
    "    # If there are more positives than negatives, sample from positives\n",
    "    if num_positive > num_negative:\n",
    "        excess_positive = num_positive - num_negative\n",
    "        X_positive_resample, y_positive_resample = resample(X_sub[y_sub == 1], y_sub[y_sub == 1],\n",
    "                                                            n_samples=num_negative, replace=False, random_state=i)\n",
    "        X_sub = pd.concat([X_positive_resample, X_sub[y_sub == 0]])\n",
    "        y_sub = pd.concat([y_positive_resample, y_sub[y_sub == 0]])\n",
    "    \n",
    "    # If there are more negatives than positives, sample from negatives\n",
    "    elif num_negative > num_positive:\n",
    "        excess_negative = num_negative - num_positive\n",
    "        X_negative_resample, y_negative_resample = resample(X_sub[y_sub == 0], y_sub[y_sub == 0],\n",
    "                                                            n_samples=num_positive, replace=False, random_state=i)\n",
    "        X_sub = pd.concat([X_sub[y_sub == 1], X_negative_resample])\n",
    "        y_sub = pd.concat([y_sub[y_sub == 1], y_negative_resample])\n",
    "    \n",
    "    # Update the sub-datasets with exactly balanced X_sub and y_sub\n",
    "    sub_X[i] = X_sub\n",
    "    sub_y[i] = y_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-train on Sub-datasets and Select the Best One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "best_f1_score = 0\n",
    "best_dataset_index = None\n",
    "results = []\n",
    "\n",
    "for i in range(3):\n",
    "    # Get the sub-dataset\n",
    "    X_sub = sub_X[i]\n",
    "    y_sub = sub_y[i]\n",
    "    \n",
    "    # Train Decision Tree\n",
    "    dt_model = DecisionTreeClassifier().fit(X_sub, y_sub)\n",
    "    dt_predictions = dt_model.predict(X_sub)\n",
    "    \n",
    "    # Train Linear Discriminant\n",
    "    lda_model = LinearDiscriminantAnalysis().fit(X_sub, y_sub)\n",
    "    lda_predictions = lda_model.predict(X_sub)\n",
    "    \n",
    "    # Calculate metrics for Decision Tree\n",
    "    dt_precision = precision_score(y_sub, dt_predictions)\n",
    "    dt_recall = recall_score(y_sub, dt_predictions)\n",
    "    dt_f1 = f1_score(y_sub, dt_predictions)\n",
    "    \n",
    "    # Calculate metrics for Linear Discriminant\n",
    "    lda_precision = precision_score(y_sub, lda_predictions)\n",
    "    lda_recall = recall_score(y_sub, lda_predictions)\n",
    "    lda_f1 = f1_score(y_sub, lda_predictions)\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'dataset': i,\n",
    "        'dt_precision': dt_precision,\n",
    "        'dt_recall': dt_recall,\n",
    "        'dt_f1': dt_f1,\n",
    "        'lda_precision': lda_precision,\n",
    "        'lda_recall': lda_recall,\n",
    "        'lda_f1': lda_f1\n",
    "    })\n",
    "    \n",
    "    # Determine the best F1 score across both models\n",
    "    best_f1 = max(dt_f1, lda_f1)\n",
    "    \n",
    "    if best_f1 > best_f1_score:\n",
    "        best_f1_score = best_f1\n",
    "        best_dataset_index = i\n",
    "\n",
    "# Output the best sub-dataset index and F1 score\n",
    "print(\"Best sub-dataset index:\", best_dataset_index)\n",
    "print(\"Best F1 score:\", best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the best sub-dataset to X_train and y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = sub_X[best_dataset_index], sub_y[best_dataset_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Train Set Target distribution after resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(y_train.value_counts())\n",
    "\n",
    "y_train.value_counts().plot(kind=\"bar\", title=\"Train Set Target Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search CV - Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use standard hyperparameters to find most suitable algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up the Models and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_quick_search = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=0),\n",
    "    \"XGBClassifier\": XGBClassifier(random_state=0),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=0),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(random_state=0),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=0),\n",
    "    \"ExtraTreesClassifier\": ExtraTreesClassifier(random_state=0),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(random_state=0),\n",
    "}\n",
    "\n",
    "params_quick_search = {\n",
    "    \"LogisticRegression\": {},\n",
    "    \"XGBClassifier\": {},\n",
    "    \"DecisionTreeClassifier\": {},\n",
    "    \"RandomForestClassifier\": {},\n",
    "    \"GradientBoostingClassifier\": {},\n",
    "    \"ExtraTreesClassifier\": {},\n",
    "    \"AdaBoostClassifier\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the custom scoring metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score, precision_score, f1_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "def npv_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fn)\n",
    "\n",
    "# Dictionary of all the metrics\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall': make_scorer(recall_score, pos_label=1),\n",
    "    'specificity': make_scorer(specificity_score),\n",
    "    'precision': make_scorer(precision_score, pos_label=1),\n",
    "    'npv': make_scorer(npv_score),\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "    'f1': make_scorer(f1_score, pos_label=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick GridSearch CV - Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
    "\n",
    "search.fit(X_train, y_train, cv=10, n_jobs=-1, scoring=scoring, refit='f1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results and sort by the F1 score\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='f1', scoring=scoring)\n",
    "grid_search_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform extensive search on the most suitable algorithm to find the best hyperparameter configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model and parameters, for Extensive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_search = {\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(random_state=0),\n",
    "}\n",
    "\n",
    "params_search = {\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"RandomForestClassifier__n_estimators\": [100, 300, 500],\n",
    "        \"RandomForestClassifier__max_depth\": [None, 10, 20],\n",
    "        \"RandomForestClassifier__min_samples_split\": [2, 10, 20],\n",
    "        \"RandomForestClassifier__min_samples_leaf\": [1, 2, 5],\n",
    "        \"RandomForestClassifier__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        \"RandomForestClassifier__bootstrap\": [True, False],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensive GridSearch CV - Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(\n",
    "    X_train, y_train, cv=10, n_jobs=-1, scoring=scoring, refit='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='f1', scoring=scoring)\n",
    "grid_search_summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best model name programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search_pipelines[best_model].best_params_\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the best clf pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
    "pipeline_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess feature importance on the current model with `.features_importances_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = pd.DataFrame(\n",
    "    data={\n",
    "        \"Feature\": X_train.columns[pipeline_clf[\"feat_selection\"].get_support()],\n",
    "        \"Importance\": pipeline_clf[\"model\"].feature_importances_,\n",
    "    }\n",
    ").sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# re-assign best_features order\n",
    "best_features = df_feature_importance[\"Feature\"].to_list()\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(\n",
    "    f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "    f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\"\n",
    ")\n",
    "\n",
    "df_feature_importance.plot(kind=\"bar\", x=\"Feature\", y=\"Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Pipeline on Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "def confusion_matrix_and_report(X, y, pipeline, label_map):\n",
    "\n",
    "    prediction = pipeline.predict(X)\n",
    "\n",
    "    print('---  Confusion Matrix  ---')\n",
    "    print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "          columns=[[\"Actual \" + sub for sub in label_map]],\n",
    "          index=[[\"Prediction \" + sub for sub in label_map]]\n",
    "          ))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print('---  Classification Report  ---')\n",
    "    print(classification_report(y, prediction, target_names=label_map), \"\\n\")\n",
    "\n",
    "\n",
    "def clf_performance(X_train, y_train, X_test, y_test, pipeline, label_map):\n",
    "    print(\"#### Train Set #### \\n\")\n",
    "    confusion_matrix_and_report(X_train, y_train, pipeline, label_map)\n",
    "\n",
    "    print(\"#### Test Set ####\\n\")\n",
    "    confusion_matrix_and_report(X_test, y_test, pipeline, label_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=pipeline_clf,\n",
    "                label_map= ['No Success', 'Success'] \n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
